{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T19:59:31.973754Z",
     "iopub.status.busy": "2024-11-18T19:59:31.973324Z",
     "iopub.status.idle": "2024-11-18T19:59:53.877588Z",
     "shell.execute_reply": "2024-11-18T19:59:53.876829Z",
     "shell.execute_reply.started": "2024-11-18T19:59:31.973710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9576e4045b548eebc8192b34be0649891ca1b7534f3a947174ae3206243e8877\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n",
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Collecting groq\n",
      "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (70.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu, groq, bert-score\n",
      "Successfully installed bert-score-0.3.13 groq-0.12.0 portalocker-3.0.0 sacrebleu-2.4.3\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk rouge-score matplotlib seaborn\n",
    "!pip install sacrebleu bert-score torchmetrics nltk rouge-score datasets transformers groq pandas tqdm matplotlib seaborn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T19:59:53.879282Z",
     "iopub.status.busy": "2024-11-18T19:59:53.878892Z",
     "iopub.status.idle": "2024-11-18T19:59:59.811641Z",
     "shell.execute_reply": "2024-11-18T19:59:59.810975Z",
     "shell.execute_reply.started": "2024-11-18T19:59:53.879248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "from bert_score import BERTScorer\n",
    "from torchmetrics.text import TranslationEditRate, WordErrorRate, CharErrorRate\n",
    "from rouge_score import rouge_scorer\n",
    "from groq import Groq\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 00:10:42,192 - INFO - Evaluating Chinese-English translations...\n",
      "2024-11-22 00:10:42,192 - INFO - Processing model: gemma2-9b-it\n",
      "2024-11-22 00:10:42,192 - INFO - Using prompt: 1\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]2024-11-22 00:10:42,764 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  1%|          | 1/100 [00:00<00:56,  1.74it/s]2024-11-22 00:10:43,359 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 2/100 [00:01<00:57,  1.71it/s]2024-11-22 00:10:44,019 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 3/100 [00:01<01:00,  1.61it/s]2024-11-22 00:10:44,743 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  4%|▍         | 4/100 [00:02<01:04,  1.50it/s]2024-11-22 00:10:45,393 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|▌         | 5/100 [00:03<01:02,  1.53it/s]2024-11-22 00:10:45,948 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  6%|▌         | 6/100 [00:03<00:58,  1.61it/s]2024-11-22 00:10:46,529 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  7%|▋         | 7/100 [00:04<00:56,  1.65it/s]2024-11-22 00:10:47,127 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  8%|▊         | 8/100 [00:04<00:55,  1.65it/s]2024-11-22 00:10:47,827 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  9%|▉         | 9/100 [00:05<00:57,  1.58it/s]2024-11-22 00:10:48,429 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|█         | 10/100 [00:06<00:56,  1.60it/s]2024-11-22 00:10:50,014 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 11%|█         | 11/100 [00:07<01:21,  1.09it/s]2024-11-22 00:10:50,664 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 12%|█▏        | 12/100 [00:08<01:14,  1.19it/s]2024-11-22 00:10:51,346 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 13%|█▎        | 13/100 [00:09<01:08,  1.27it/s]2024-11-22 00:10:51,900 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 14%|█▍        | 14/100 [00:09<01:02,  1.38it/s]2024-11-22 00:10:52,534 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 15%|█▌        | 15/100 [00:10<00:58,  1.45it/s]2024-11-22 00:10:53,113 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 16%|█▌        | 16/100 [00:10<00:55,  1.52it/s]2024-11-22 00:10:53,801 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 17/100 [00:11<00:55,  1.50it/s]2024-11-22 00:10:54,404 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 18%|█▊        | 18/100 [00:12<00:53,  1.54it/s]2024-11-22 00:10:55,036 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 19%|█▉        | 19/100 [00:12<00:52,  1.56it/s]2024-11-22 00:10:55,716 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 20%|██        | 20/100 [00:13<00:52,  1.52it/s]2024-11-22 00:10:57,286 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 21%|██        | 21/100 [00:15<01:13,  1.08it/s]2024-11-22 00:10:57,984 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 22/100 [00:15<01:07,  1.16it/s]2024-11-22 00:10:58,538 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:10:58,538 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:00,891 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 23%|██▎       | 23/100 [00:18<01:53,  1.47s/it]2024-11-22 00:11:01,342 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:01,357 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:03,743 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 24%|██▍       | 24/100 [00:21<02:23,  1.89s/it]2024-11-22 00:11:04,312 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:04,314 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:06,895 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 25/100 [00:24<02:49,  2.27s/it]2024-11-22 00:11:07,381 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:07,381 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:09,769 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 26%|██▌       | 26/100 [00:27<03:01,  2.45s/it]2024-11-22 00:11:10,317 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:10,333 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:12,750 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 27%|██▋       | 27/100 [00:30<03:10,  2.61s/it]2024-11-22 00:11:13,236 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:13,236 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:15,570 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 28%|██▊       | 28/100 [00:33<03:12,  2.67s/it]2024-11-22 00:11:16,072 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:16,072 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:18,476 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 29%|██▉       | 29/100 [00:36<03:14,  2.74s/it]2024-11-22 00:11:18,980 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:18,981 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:21,358 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 30%|███       | 30/100 [00:39<03:15,  2.79s/it]2024-11-22 00:11:22,865 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:22,865 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:11:24,264 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 31%|███       | 31/100 [00:42<03:14,  2.82s/it]2024-11-22 00:11:24,839 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:24,840 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:27,213 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 32%|███▏      | 32/100 [00:45<03:14,  2.86s/it]2024-11-22 00:11:27,784 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:27,784 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:30,203 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 33/100 [00:48<03:14,  2.90s/it]2024-11-22 00:11:30,707 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:30,707 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:33,090 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 34%|███▍      | 34/100 [00:50<03:11,  2.89s/it]2024-11-22 00:11:33,642 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:33,642 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:36,027 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 35%|███▌      | 35/100 [00:53<03:08,  2.91s/it]2024-11-22 00:11:36,594 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:36,595 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:38,966 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 36%|███▌      | 36/100 [00:56<03:06,  2.92s/it]2024-11-22 00:11:39,434 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:39,434 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:41,765 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 37%|███▋      | 37/100 [00:59<03:01,  2.88s/it]2024-11-22 00:11:42,316 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:42,317 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:44,651 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 38%|███▊      | 38/100 [01:02<02:58,  2.88s/it]2024-11-22 00:11:45,135 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:45,135 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:47,474 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 39%|███▉      | 39/100 [01:05<02:54,  2.86s/it]2024-11-22 00:11:48,022 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:48,022 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:50,340 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 40/100 [01:08<02:52,  2.87s/it]2024-11-22 00:11:51,828 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:51,828 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:11:53,210 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 41%|████      | 41/100 [01:11<02:49,  2.87s/it]2024-11-22 00:11:53,893 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:53,894 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:56,265 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 42%|████▏     | 42/100 [01:14<02:49,  2.92s/it]2024-11-22 00:11:56,748 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:56,748 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:11:59,132 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 43%|████▎     | 43/100 [01:16<02:45,  2.91s/it]2024-11-22 00:11:59,600 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:11:59,600 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:01,953 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 44%|████▍     | 44/100 [01:19<02:41,  2.88s/it]2024-11-22 00:12:02,436 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:02,451 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:04,855 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 45%|████▌     | 45/100 [01:22<02:38,  2.89s/it]2024-11-22 00:12:05,339 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:05,339 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:07,692 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 46%|████▌     | 46/100 [01:25<02:35,  2.87s/it]2024-11-22 00:12:08,175 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:08,175 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:10,509 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 47%|████▋     | 47/100 [01:28<02:31,  2.86s/it]2024-11-22 00:12:11,061 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:11,061 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:13,347 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 48%|████▊     | 48/100 [01:31<02:28,  2.85s/it]2024-11-22 00:12:13,913 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:13,914 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:16,249 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 49%|████▉     | 49/100 [01:34<02:26,  2.86s/it]2024-11-22 00:12:16,784 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:16,784 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:19,080 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 50/100 [01:36<02:22,  2.85s/it]2024-11-22 00:12:20,568 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:20,568 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:12:21,885 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 51%|█████     | 51/100 [01:39<02:19,  2.84s/it]2024-11-22 00:12:22,358 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:22,358 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:24,742 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 52%|█████▏    | 52/100 [01:42<02:16,  2.84s/it]2024-11-22 00:12:25,291 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:25,291 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:27,578 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 53%|█████▎    | 53/100 [01:45<02:13,  2.84s/it]2024-11-22 00:12:28,062 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:28,077 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:30,431 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 54%|█████▍    | 54/100 [01:48<02:10,  2.85s/it]2024-11-22 00:12:30,946 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:30,946 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:33,319 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 55%|█████▌    | 55/100 [01:51<02:08,  2.86s/it]2024-11-22 00:12:33,852 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:33,852 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:36,201 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 56%|█████▌    | 56/100 [01:54<02:06,  2.87s/it]2024-11-22 00:12:36,702 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:36,702 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:39,120 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 57%|█████▋    | 57/100 [01:56<02:03,  2.88s/it]2024-11-22 00:12:39,691 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:39,691 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:42,024 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 58%|█████▊    | 58/100 [01:59<02:01,  2.89s/it]2024-11-22 00:12:42,509 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:42,509 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:44,858 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 59%|█████▉    | 59/100 [02:02<01:57,  2.87s/it]2024-11-22 00:12:45,328 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:45,328 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:47,763 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 60%|██████    | 60/100 [02:05<01:55,  2.88s/it]2024-11-22 00:12:49,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:49,332 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:12:50,653 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 61%|██████    | 61/100 [02:08<01:52,  2.88s/it]2024-11-22 00:12:51,116 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:51,116 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:53,482 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 62%|██████▏   | 62/100 [02:11<01:48,  2.87s/it]2024-11-22 00:12:53,953 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:53,953 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:56,338 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 63%|██████▎   | 63/100 [02:14<01:45,  2.86s/it]2024-11-22 00:12:56,824 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:56,824 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:12:59,159 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 64%|██████▍   | 64/100 [02:16<01:42,  2.85s/it]2024-11-22 00:12:59,706 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:12:59,706 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:02,040 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 65%|██████▌   | 65/100 [02:19<01:40,  2.86s/it]2024-11-22 00:13:02,527 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:02,527 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:04,930 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 66%|██████▌   | 66/100 [02:22<01:37,  2.87s/it]2024-11-22 00:13:05,431 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:05,431 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:07,803 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 67/100 [02:25<01:34,  2.87s/it]2024-11-22 00:13:08,283 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:08,283 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:10,684 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 68%|██████▊   | 68/100 [02:28<01:31,  2.87s/it]2024-11-22 00:13:11,170 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:11,170 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:13,573 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▉   | 69/100 [02:31<01:29,  2.88s/it]2024-11-22 00:13:14,058 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:14,058 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:16,409 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 70%|███████   | 70/100 [02:34<01:25,  2.87s/it]2024-11-22 00:13:17,941 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:17,941 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:13:19,631 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 71%|███████   | 71/100 [02:37<01:26,  2.97s/it]2024-11-22 00:13:20,111 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:20,111 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:22,546 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 72%|███████▏  | 72/100 [02:40<01:22,  2.96s/it]2024-11-22 00:13:23,016 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:23,016 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:25,437 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 73%|███████▎  | 73/100 [02:43<01:19,  2.94s/it]2024-11-22 00:13:25,916 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:25,916 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:28,272 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 74%|███████▍  | 74/100 [02:46<01:15,  2.91s/it]2024-11-22 00:13:28,752 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:28,752 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:31,107 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 75%|███████▌  | 75/100 [02:48<01:12,  2.88s/it]2024-11-22 00:13:31,677 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:31,677 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:34,010 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 76%|███████▌  | 76/100 [02:51<01:09,  2.89s/it]2024-11-22 00:13:34,491 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:34,491 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:36,846 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 77%|███████▋  | 77/100 [02:54<01:06,  2.87s/it]2024-11-22 00:13:37,342 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:37,342 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:39,714 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 78/100 [02:57<01:03,  2.87s/it]2024-11-22 00:13:40,184 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:40,184 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:42,520 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 79%|███████▉  | 79/100 [03:00<00:59,  2.85s/it]2024-11-22 00:13:42,984 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:42,984 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:45,286 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 80/100 [03:03<00:56,  2.83s/it]2024-11-22 00:13:46,775 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:46,775 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:13:48,105 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 81%|████████  | 81/100 [03:05<00:53,  2.82s/it]2024-11-22 00:13:48,576 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:48,576 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:50,894 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 82%|████████▏ | 82/100 [03:08<00:50,  2.81s/it]2024-11-22 00:13:51,379 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:51,379 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:53,728 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 83/100 [03:11<00:47,  2.82s/it]2024-11-22 00:13:54,777 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:54,777 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:13:56,167 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 84%|████████▍ | 84/100 [03:13<00:43,  2.71s/it]2024-11-22 00:13:56,668 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:56,668 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:13:58,985 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 85%|████████▌ | 85/100 [03:16<00:41,  2.74s/it]2024-11-22 00:13:59,469 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:13:59,470 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:01,806 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 86%|████████▌ | 86/100 [03:19<00:38,  2.76s/it]2024-11-22 00:14:02,339 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:02,339 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:04,688 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 87%|████████▋ | 87/100 [03:22<00:36,  2.80s/it]2024-11-22 00:14:05,221 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:05,221 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:07,561 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 88%|████████▊ | 88/100 [03:25<00:33,  2.82s/it]2024-11-22 00:14:08,041 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:08,041 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:10,365 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▉ | 89/100 [03:28<00:31,  2.82s/it]2024-11-22 00:14:10,845 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:10,845 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:13,167 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 90%|█████████ | 90/100 [03:30<00:28,  2.81s/it]2024-11-22 00:14:14,735 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:14,735 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2024-11-22 00:14:16,118 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 91%|█████████ | 91/100 [03:33<00:25,  2.85s/it]2024-11-22 00:14:16,588 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:16,588 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:18,907 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 92%|█████████▏| 92/100 [03:36<00:22,  2.83s/it]2024-11-22 00:14:19,403 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:19,403 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:21,742 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 93%|█████████▎| 93/100 [03:39<00:19,  2.83s/it]2024-11-22 00:14:22,376 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:22,376 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:24,726 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 94%|█████████▍| 94/100 [03:42<00:17,  2.88s/it]2024-11-22 00:14:25,227 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:25,227 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:27,598 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 95%|█████████▌| 95/100 [03:45<00:14,  2.88s/it]2024-11-22 00:14:28,062 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:28,062 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:30,428 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 96%|█████████▌| 96/100 [03:48<00:11,  2.86s/it]2024-11-22 00:14:31,000 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:31,000 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:33,322 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 97%|█████████▋| 97/100 [03:51<00:08,  2.87s/it]2024-11-22 00:14:33,817 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:33,817 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:36,124 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 98%|█████████▊| 98/100 [03:53<00:05,  2.86s/it]2024-11-22 00:14:36,604 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:36,604 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:38,944 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 99%|█████████▉| 99/100 [03:56<00:02,  2.84s/it]2024-11-22 00:14:39,439 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-22 00:14:39,439 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2024-11-22 00:14:41,780 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 100/100 [03:59<00:00,  2.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 393\u001b[0m\n\u001b[0;32m    390\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(results)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 393\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 386\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    384\u001b[0m source_lang, target_lang \u001b[38;5;241m=\u001b[39m pair_code\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    385\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m translations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 386\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmt_evaluation_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    388\u001b[0m all_results[pair_code] \u001b[38;5;241m=\u001b[39m results\n",
      "Cell \u001b[1;32mIn[6], line 270\u001b[0m, in \u001b[0;36mevaluate_models\u001b[1;34m(dataset, source_lang, target_lang, prompts)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m translations:\n\u001b[1;32m--> 270\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     model_prompt_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) - Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    272\u001b[0m     results[model_prompt_key] \u001b[38;5;241m=\u001b[39m metrics\n",
      "Cell \u001b[1;32mIn[6], line 176\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(references, hypotheses)\u001b[0m\n\u001b[0;32m    174\u001b[0m chrf \u001b[38;5;241m=\u001b[39m CHRF()\n\u001b[0;32m    175\u001b[0m ter_metric \u001b[38;5;241m=\u001b[39m TER()\n\u001b[1;32m--> 176\u001b[0m bert_scorer \u001b[38;5;241m=\u001b[39m \u001b[43mBERTScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale_with_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m wer \u001b[38;5;241m=\u001b[39m WordErrorRate()\n\u001b[0;32m    178\u001b[0m cer \u001b[38;5;241m=\u001b[39m CharErrorRate()\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\bert_score\\scorer.py:98\u001b[0m, in \u001b[0;36mBERTScorer.__init__\u001b[1;34m(self, model_type, num_layers, batch_size, nthreads, all_layers, idf, idf_sents, device, lang, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_fast_tokenizer \u001b[38;5;241m=\u001b[39m use_fast_tokenizer\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_fast_tokenizer)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idf_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\bert_score\\utils.py:255\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(model_type, num_layers, all_layers)\u001b[0m\n\u001b[0;32m    253\u001b[0m     model \u001b[38;5;241m=\u001b[39m T5EncoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_type)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3809\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3793\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3794\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3795\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3808\u001b[0m     }\n\u001b[1;32m-> 3809\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3814\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    860\u001b[0m     )\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1545\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1542\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1543\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1545\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1554\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:454\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    452\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[0;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\bubha\\Downloads\\fydp\\.venv\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from groq import Groq\n",
    "\n",
    "# Metrics imports\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "from bert_score import BERTScorer\n",
    "from torchmetrics.text import WordErrorRate, CharErrorRate\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize Groq clients with multiple API keys\n",
    "API_KEYS = [\n",
    "    \"your_api_key_1\"\n",
    "]\n",
    "\n",
    "# API configuration\n",
    "API_CONFIG = {\n",
    "    \"calls_per_minute\": 50,  # Adjust based on API limits\n",
    "    \"timeout\": 30,\n",
    "    \"max_retries\": 3,\n",
    "    \"backoff_factor\": 2\n",
    "}\n",
    "\n",
    "class APIManager:\n",
    "    def __init__(self, api_keys):\n",
    "        self.api_keys = api_keys\n",
    "        self.current_key_index = 0\n",
    "        self.request_counts = {key: 0 for key in api_keys}\n",
    "        self.last_request_time = time.time()\n",
    "        self.error_counts = {key: 0 for key in api_keys}\n",
    "        \n",
    "    def get_current_key(self):\n",
    "        return self.api_keys[self.current_key_index]\n",
    "        \n",
    "    def rotate_key(self):\n",
    "        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)\n",
    "        logging.info(f\"Rotating to API key index: {self.current_key_index}\")\n",
    "        return self.get_current_key()\n",
    "        \n",
    "    def handle_error(self, error):\n",
    "        current_key = self.get_current_key()\n",
    "        self.error_counts[current_key] += 1\n",
    "        \n",
    "        if \"rate limit\" in str(error).lower() or self.error_counts[current_key] >= 3:\n",
    "            self.error_counts[current_key] = 0\n",
    "            return self.rotate_key()\n",
    "        return current_key\n",
    "\n",
    "# Initialize API manager\n",
    "api_manager = APIManager(API_KEYS)\n",
    "\n",
    "# Define models with their context lengths\n",
    "MODELS = {\n",
    "    \"gemma2-9b-it\": {\"provider\": \"Google\", \"context_length\": 8192},\n",
    "    \"gemma-7b-it\": {\"provider\": \"Google\", \"context_length\": 8192},\n",
    "    \"llama3-groq-70b-8192-tool-use-preview\": {\"provider\": \"Groq\", \"context_length\": 8192},\n",
    "    \"llama3-groq-8b-8192-tool-use-preview\": {\"provider\": \"Groq\", \"context_length\": 8192},\n",
    "    \"llama-3.1-70b-versatile\": {\"provider\": \"Meta\", \"context_length\": 8192},\n",
    "    \"llama-3.1-8b-instant\": {\"provider\": \"Meta\", \"context_length\": 8192},\n",
    "    \"mixtral-8x7b-32768\": {\"provider\": \"Mistral\", \"context_length\": 32768},\n",
    "    \"llama-3.2-90b-vision-preview\": {\"provider\": \"Meta\", \"context_length\": 128000}\n",
    "}\n",
    "\n",
    "class APIKeyLimitError(Exception):\n",
    "    pass\n",
    "\n",
    "class TranslationError(Exception):\n",
    "    \"\"\"Custom exception for translation errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def switch_client():\n",
    "    global client_index, client\n",
    "    client_index = (client_index + 1) % len(API_KEYS)\n",
    "    client = Groq(api_key=API_KEYS[client_index])\n",
    "    logging.info(f\"Switched to API key index: {client_index}\")\n",
    "\n",
    "def load_translation_data(language_pair, num_samples=1000):\n",
    "    \"\"\"Load dataset for specified language pair.\"\"\"\n",
    "    try:\n",
    "        # Try loading from validation set first\n",
    "        dataset = load_dataset(\"wmt19\", language_pair, split=\"validation\", streaming=True)\n",
    "    except ValueError:\n",
    "        # If validation not available, try train set\n",
    "        dataset = load_dataset(\"wmt19\", language_pair, split=\"train\", streaming=True)\n",
    "    \n",
    "    # Select the specified number of samples\n",
    "    dataset = dataset.take(num_samples)\n",
    "    return list(dataset)\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=API_CONFIG[\"calls_per_minute\"], period=60)\n",
    "@retry(stop=stop_after_attempt(3), \n",
    "       wait=wait_exponential(multiplier=API_CONFIG[\"backoff_factor\"], min=4, max=10),\n",
    "       retry=retry_if_exception_type((APIKeyLimitError, TranslationError)))\n",
    "def translate_text(text, model_name, source_lang, target_lang, prompt):\n",
    "    \"\"\"Enhanced translation function with improved API handling\"\"\"\n",
    "    try:\n",
    "        current_key = api_manager.get_current_key()\n",
    "        client = Groq(api_key=current_key)\n",
    "        \n",
    "        language_names = {\n",
    "            'cs': 'Czech',\n",
    "            'en': 'English',\n",
    "            'de': 'German',\n",
    "            'fi': 'Finnish',\n",
    "            'fr': 'French',\n",
    "            'gu': 'Gujarati',\n",
    "            'kk': 'Kazakh',\n",
    "            'lt': 'Lithuanian',\n",
    "            'ru': 'Russian',\n",
    "            'zh': 'Chinese'\n",
    "        }\n",
    "        \n",
    "        source_lang_name = language_names[source_lang]\n",
    "        target_lang_name = language_names[target_lang]\n",
    "            \n",
    "        # Get model's max context length\n",
    "        max_length = MODELS[model_name][\"context_length\"]\n",
    "        \n",
    "        # Truncate input if necessary to fit context length (leaving room for prompt and response)\n",
    "        safe_length = max_length - 500  # Reserve tokens for prompt and response\n",
    "        if len(text) > safe_length:\n",
    "            text = text[:safe_length] + \"...\"\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional translator focused on accuracy and fluency.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            timeout=API_CONFIG[\"timeout\"]\n",
    "        )\n",
    "        \n",
    "        translation = chat_completion.choices[0].message.content.strip()\n",
    "        if not translation or len(translation) < 2:\n",
    "            raise TranslationError(\"Empty or invalid translation received\")\n",
    "            \n",
    "        # Track successful request\n",
    "        api_manager.request_counts[current_key] += 1\n",
    "        return translation\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Translation error with key {api_manager.current_key_index}: {str(e)}\")\n",
    "        new_key = api_manager.handle_error(e)\n",
    "        if new_key != current_key:\n",
    "            raise APIKeyLimitError(\"Rotating API key due to errors\")\n",
    "        raise TranslationError(str(e))\n",
    "\n",
    "def calculate_metrics(references, hypotheses):\n",
    "    \"\"\"Calculate various MT evaluation metrics.\"\"\"\n",
    "    try:\n",
    "        # Initialize metrics\n",
    "        bleu = BLEU()\n",
    "        chrf = CHRF()\n",
    "        ter_metric = TER()\n",
    "        bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "        wer = WordErrorRate()\n",
    "        cer = CharErrorRate()\n",
    "        rouge_metrics = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        # Ensure inputs are valid\n",
    "        if not references or not hypotheses:\n",
    "            raise ValueError(\"Empty references or hypotheses\")\n",
    "            \n",
    "        # Calculate metrics with error handling\n",
    "        try:\n",
    "            bleu_score = bleu.corpus_score(hypotheses, [references]).score\n",
    "            chrf_score = chrf.corpus_score(hypotheses, [references]).score\n",
    "            ter_score = ter_metric.corpus_score(hypotheses, [references]).score\n",
    "            \n",
    "            # BERTScore calculation\n",
    "            P, R, F1 = bert_scorer.score(hypotheses, references)\n",
    "            bert_score = F1.mean().item()\n",
    "            \n",
    "            # WER and CER calculation\n",
    "            wer_score = wer(hypotheses, references)\n",
    "            cer_score = cer(hypotheses, references)\n",
    "            \n",
    "            # ROUGE scores calculation\n",
    "            rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                scores = rouge_metrics.score(ref, hyp)\n",
    "                rouge_scores['rouge1'] += scores['rouge1'].fmeasure\n",
    "                rouge_scores['rouge2'] += scores['rouge2'].fmeasure\n",
    "                rouge_scores['rougeL'] += scores['rougeL'].fmeasure\n",
    "            \n",
    "            for key in rouge_scores:\n",
    "                rouge_scores[key] /= len(hypotheses)\n",
    "                \n",
    "            return {\n",
    "                \"BLEU\": bleu_score,\n",
    "                \"chrF\": chrf_score,\n",
    "                \"TER\": ter_score,\n",
    "                \"BERTScore\": bert_score,\n",
    "                \"WER\": wer_score,\n",
    "                \"CER\": cer_score,\n",
    "                \"ROUGE-1\": rouge_scores['rouge1'],\n",
    "                \"ROUGE-2\": rouge_scores['rouge2'],\n",
    "                \"ROUGE-L\": rouge_scores['rougeL']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating metrics: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing metrics: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_translations(model_name, source_lang, target_lang, prompt_name, source_texts, references, translations):\n",
    "    pd.DataFrame({\n",
    "        'Source': source_texts,\n",
    "        'Reference': references,\n",
    "        'Translation': translations\n",
    "    }).to_csv(f'translations_{model_name}_{source_lang}-{target_lang}_{prompt_name}.csv', index=False)\n",
    "\n",
    "def evaluate_models(dataset, source_lang, target_lang, prompts):\n",
    "    \"\"\"Enhanced evaluation function with better API handling\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_info in MODELS.items():\n",
    "        logging.info(f\"Processing model: {model_name}\")\n",
    "        \n",
    "        for prompt_id, prompt_func in prompts:\n",
    "            logging.info(f\"Using prompt: {prompt_id}\")\n",
    "            translations = []\n",
    "            references = []\n",
    "            source_texts = []\n",
    "            \n",
    "            for example in tqdm(dataset):\n",
    "                try:\n",
    "                    # Add delay between requests if needed\n",
    "                    if len(translations) > 0 and len(translations) % 10 == 0:\n",
    "                        time.sleep(1)  # Prevent hitting rate limits\n",
    "                    \n",
    "                    source_text = example['translation'][source_lang]\n",
    "                    reference = example['translation'][target_lang]\n",
    "                    prompt = prompt_func(source_text, source_lang, target_lang)\n",
    "                    translation = translate_text(source_text, model_name, source_lang, target_lang, prompt)\n",
    "                    \n",
    "                    source_texts.append(source_text)\n",
    "                    translations.append(translation)\n",
    "                    references.append(reference)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing example: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if translations:\n",
    "                metrics = calculate_metrics(references, translations)\n",
    "                model_prompt_key = f\"{model_name} ({model_info['provider']}) - Prompt {prompt_id}\"\n",
    "                results[model_prompt_key] = metrics\n",
    "                \n",
    "                save_translations(\n",
    "                    model_name=model_name,\n",
    "                    source_lang=source_lang,\n",
    "                    target_lang=target_lang,\n",
    "                    prompt_name=f\"Prompt_{prompt_id}\",\n",
    "                    source_texts=source_texts,\n",
    "                    references=references,\n",
    "                    translations=translations\n",
    "                )\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "def visualize_results(results, pair_name):\n",
    "    \"\"\"Create visualizations for the evaluation results.\"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(results, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "    plt.title(f'Translation Metrics Comparison for {pair_name}')\n",
    "    plt.ylabel('Models')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'mt_evaluation_heatmap_{pair_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    language_pairs = {\n",
    "    # (\"cs-en\"): [(\"cs-en\", \"Czech-English\"), (\"en-cs\", \"English-Czech\")],\n",
    "    # (\"de-en\"): [(\"de-en\", \"German-English\"), (\"en-de\", \"English-German\")],\n",
    "    # (\"fi-en\"): [(\"fi-en\", \"Finnish-English\"), (\"en-fi\", \"English-Finnish\")],\n",
    "    # (\"fr-de\"): [(\"fr-de\", \"French-German\"), (\"de-fr\", \"German-French\")],\n",
    "    # (\"gu-en\"): [(\"gu-en\", \"Gujarati-English\"), (\"en-gu\", \"English-Gujarati\")],\n",
    "    # (\"kk-en\"): [(\"kk-en\", \"Kazakh-English\"), (\"en-kk\", \"English-Kazakh\")],\n",
    "    # (\"lt-en\"): [(\"lt-en\", \"Lithuanian-English\"), (\"en-lt\", \"English-Lithuanian\")],\n",
    "    # (\"ru-en\"): [(\"ru-en\", \"Russian-English\"), (\"en-ru\", \"English-Russian\")],\n",
    "    (\"zh-en\"): [(\"zh-en\", \"Chinese-English\"), (\"en-zh\", \"English-Chinese\")]\n",
    "    }\n",
    "\n",
    "    prompts = [\n",
    "        (1, lambda text, sl, tl: f\"\"\"System: Professional {sl}-{tl} translator\n",
    "        Objective: Precise translation maintaining source meaning and target fluency\n",
    "        Guidelines:\n",
    "        1. Preserve exact meaning\n",
    "        2. Maintain formatting\n",
    "        3. Keep technical terms\n",
    "        4. Ensure natural flow\n",
    "        \n",
    "        IMPORTANT: Provide ONLY the translation, without explanations or additional text.\n",
    "        \n",
    "        Source ({sl}): {text}\n",
    "        Translation ({tl}): \"\"\"),\n",
    "\n",
    "        (2, lambda text, sl, tl: f\"\"\"System: Expert {sl}-{tl} translator with deep cultural understanding.\n",
    "        Context: Professional translation requiring cultural and contextual accuracy.\n",
    "        Requirements:\n",
    "        - Preserve idiomatic expressions\n",
    "        - Adapt cultural references appropriately\n",
    "        - Maintain tone and register\n",
    "        - Ensure natural {tl} language patterns\n",
    "        \n",
    "        IMPORTANT: Return ONLY the translated text, nothing else.\n",
    "        \n",
    "        Original ({sl}): {text}\n",
    "        Translation ({tl}): \"\"\"),\n",
    "\n",
    "        (3, lambda text, sl, tl: f\"\"\"System: Specialized translation engine for {sl} to {tl}.\n",
    "        Focus areas:\n",
    "        - Technical accuracy\n",
    "        - Domain-specific terminology\n",
    "        - Structural equivalence\n",
    "        - Target language conventions\n",
    "        \n",
    "        IMPORTANT: Output ONLY the translation, no comments or explanations.\n",
    "        \n",
    "        Input ({sl}): {text}\n",
    "        Professional translation ({tl}): \"\"\"),\n",
    "\n",
    "        (4, lambda text, sl, tl: f\"\"\"System: Neural machine translation model optimized for {sl}-{tl} pair.\n",
    "        Translation parameters:\n",
    "        - Maximum semantic fidelity\n",
    "        - Context preservation\n",
    "        - Appropriate register\n",
    "        - Natural language generation\n",
    "        \n",
    "        IMPORTANT: Respond with ONLY the translation text.\n",
    "        \n",
    "        Source content ({sl}): {text}\n",
    "        Target content ({tl}): \"\"\"),\n",
    "\n",
    "        (5, lambda text, sl, tl: f\"\"\"System: Professional translation service\n",
    "        Task: Convert from {sl} to {tl}\n",
    "        Requirements:\n",
    "        - Highest accuracy level\n",
    "        - Natural expression\n",
    "        - Contextual awareness\n",
    "        - Style matching\n",
    "        \n",
    "        IMPORTANT: Give ONLY the translation, without any additional text.\n",
    "        \n",
    "        Source text ({sl}):\n",
    "        {text}\n",
    "        \n",
    "        High-quality translation ({tl}):\"\"\")\n",
    "    ]\n",
    "\n",
    "    all_results = {}\n",
    "    \n",
    "    for pair_name, lang_pairs in language_pairs.items():\n",
    "        dataset = load_translation_data(pair_name, num_samples=100)\n",
    "        for pair_code, name in lang_pairs:\n",
    "            source_lang, target_lang = pair_code.split(\"-\")\n",
    "            logging.info(f\"Evaluating {name} translations...\")\n",
    "            results = evaluate_models(dataset, source_lang, target_lang, prompts)\n",
    "            results.to_csv(f\"mt_evaluation_results_{pair_code}.csv\")\n",
    "            all_results[pair_code] = results\n",
    "            logging.info(f\"Results for {name}:\")\n",
    "            logging.info(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
