{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "# # Model and tokenizer names\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "#\n",
    "# # Load the model and tokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#\n",
    "# # Function to generate text with long context\n",
    "# def generate_text_with_long_context(prompt, max_length=100):\n",
    "#     # Tokenize the prompt and ensure it fits within the context length\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128000)\n",
    "#     outputs = model.generate(**inputs, max_length=max_length)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#\n",
    "# # Example usage with a longer prompt\n",
    "# long_prompt = \"\"\"\n",
    "# This is a very long piece of text that needs to be summarized.\n",
    "# It includes multiple paragraphs and detailed information.\n",
    "# The model should be able to handle this context length effectively.\n",
    "# \"\"\"\n",
    "# generated_text = generate_text_with_long_context(long_prompt)\n",
    "# print(generated_text)"
   ],
   "id": "1033963b2d2466c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "779f657a5cc6ed79"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
