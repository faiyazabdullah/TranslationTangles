{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score matplotlib seaborn"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T21:29:17.487114Z",
          "iopub.execute_input": "2024-11-07T21:29:17.487751Z",
          "iopub.status.idle": "2024-11-07T21:29:32.743520Z",
          "shell.execute_reply.started": "2024-11-07T21:29:17.487704Z",
          "shell.execute_reply": "2024-11-07T21:29:32.742255Z"
        },
        "trusted": true,
        "id": "HR2idxdQRbxk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this cell at the beginning of your notebook\n",
        "\n",
        "!pip install sacrebleu bert-score torchmetrics nltk rouge-score datasets transformers groq pandas tqdm matplotlib seaborn\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Optional: if you want to check installed versions\n",
        "\n",
        "import pkg_resources\n",
        "packages = ['sacrebleu', 'bert_score', 'torchmetrics', 'nltk', 'rouge_score',\n",
        "\n",
        "           'datasets', 'transformers', 'groq', 'pandas', 'tqdm']\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        version = pkg_resources.get_distribution(package).version\n",
        "        print(f\"{package}: {version}\")\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        print(f\"{package}: Not installed\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T21:29:32.745948Z",
          "iopub.execute_input": "2024-11-07T21:29:32.746310Z",
          "iopub.status.idle": "2024-11-07T21:29:48.361272Z",
          "shell.execute_reply.started": "2024-11-07T21:29:32.746272Z",
          "shell.execute_reply": "2024-11-07T21:29:48.360347Z"
        },
        "trusted": true,
        "id": "IV0m33f3Rbxm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "from bert_score import BERTScorer\n",
        "from torchmetrics.text import TranslationEditRate, WordErrorRate, CharErrorRate\n",
        "from rouge_score import rouge_scorer\n",
        "from groq import Groq\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize Groq client with placeholder API key\n",
        "client = Groq(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# Define models with their context lengths\n",
        "MODELS = {\n",
        "    \"gemma2-9b-it\": {\"provider\": \"Google\", \"context_length\": 8192},\n",
        "    \"gemma-7b-it\": {\"provider\": \"Google\", \"context_length\": 8192},\n",
        "    \"llama3-groq-70b-8192-tool-use-preview\": {\"provider\": \"Groq\", \"context_length\": 8192},\n",
        "    \"llama3-groq-8b-8192-tool-use-preview\": {\"provider\": \"Groq\", \"context_length\": 8192},\n",
        "    \"llama-3.1-70b-versatile\": {\"provider\": \"Meta\", \"context_length\": 8192},\n",
        "    \"llama-3.1-8b-instant\": {\"provider\": \"Meta\", \"context_length\": 8192},\n",
        "    \"mixtral-8x7b-32768\": {\"provider\": \"Mistral\", \"context_length\": 32768},\n",
        "    \"llama-3.2-90b-vision-preview\": {\"provider\": \"Meta\", \"context_length\": 128000}\n",
        "}\n",
        "\n",
        "def load_translation_data(language_pair, num_samples=20):\n",
        "    \"\"\"Load dataset for specified language pair.\"\"\"\n",
        "    try:\n",
        "        # Try loading from validation set first\n",
        "        dataset = load_dataset(\"wmt19\", language_pair, split=\"validation\")\n",
        "    except ValueError:\n",
        "        # If validation not available, try train set\n",
        "        dataset = load_dataset(\"wmt19\", language_pair, split=\"train\")\n",
        "\n",
        "    # Select the specified number of samples\n",
        "    return dataset.select(range(min(num_samples, len(dataset))))\n",
        "\n",
        "def translate_text(text, model_name, source_lang, target_lang):\n",
        "    \"\"\"Translate text using specified model via Groq API.\"\"\"\n",
        "    language_names = {\n",
        "    'cs': 'Czech',\n",
        "    'en': 'English',\n",
        "    'de': 'German',\n",
        "    'fi': 'Finnish',\n",
        "    'fr': 'French',\n",
        "    'gu': 'Gujarati',\n",
        "    'kk': 'Kazakh',\n",
        "    'lt': 'Lithuanian',\n",
        "    'ru': 'Russian',\n",
        "    'zh': 'Chinese'\n",
        "    }\n",
        "\n",
        "    source_lang_name = language_names[source_lang]\n",
        "    target_lang_name = language_names[target_lang]\n",
        "\n",
        "    prompt = f\"\"\"Translate the following {source_lang_name} text to {target_lang_name}:\n",
        "    {text}\n",
        "    Translation:\"\"\"\n",
        "\n",
        "    # Get model's max context length\n",
        "    max_length = MODELS[model_name][\"context_length\"]\n",
        "\n",
        "    # Truncate input if necessary to fit context length (leaving room for prompt and response)\n",
        "    safe_length = max_length - 500  # Reserve tokens for prompt and response\n",
        "    if len(text) > safe_length:\n",
        "        text = text[:safe_length] + \"...\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "def calculate_metrics(references, hypotheses):\n",
        "    \"\"\"Calculate various MT evaluation metrics.\"\"\"\n",
        "    # Initialize metrics\n",
        "    bleu = BLEU()\n",
        "    chrf = CHRF()\n",
        "    ter_metric = TER()\n",
        "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
        "    wer = WordErrorRate()\n",
        "    cer = CharErrorRate()\n",
        "    rouge_metrics = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate BLEU and variants\n",
        "    bleu_score = bleu.corpus_score(hypotheses, [references]).score\n",
        "    chrf_score = chrf.corpus_score(hypotheses, [references]).score\n",
        "    ter_score = ter_metric.corpus_score(hypotheses, [references]).score\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    P, R, F1 = bert_scorer.score(hypotheses, references)\n",
        "    bert_score = F1.mean().item()\n",
        "\n",
        "    # Calculate WER and CER\n",
        "    wer_score = wer(hypotheses, references).item()\n",
        "    cer_score = cer(hypotheses, references).item()\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "    for hyp, ref in zip(hypotheses, references):\n",
        "        scores = rouge_metrics.score(ref, hyp)\n",
        "        rouge_scores['rouge1'] += scores['rouge1'].fmeasure\n",
        "        rouge_scores['rouge2'] += scores['rouge2'].fmeasure\n",
        "        rouge_scores['rougeL'] += scores['rougeL'].fmeasure\n",
        "\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] /= len(hypotheses)\n",
        "\n",
        "    return {\n",
        "        \"BLEU\": bleu_score,\n",
        "        \"chrF\": chrf_score,\n",
        "        \"TER\": ter_score,\n",
        "        \"BERTScore\": bert_score,\n",
        "        \"WER\": wer_score,\n",
        "        \"CER\": cer_score,\n",
        "        \"ROUGE-1\": rouge_scores['rouge1'],\n",
        "        \"ROUGE-2\": rouge_scores['rouge2'],\n",
        "        \"ROUGE-L\": rouge_scores['rougeL']\n",
        "    }\n",
        "\n",
        "def evaluate_models(dataset, source_lang, target_lang):\n",
        "    \"\"\"Evaluate multiple models on the dataset.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for model_name, model_info in MODELS.items():\n",
        "        print(f\"\\nEvaluating {model_name} ({model_info['provider']})...\")\n",
        "        translations = []\n",
        "        references = []\n",
        "        source_texts = []\n",
        "\n",
        "        for example in tqdm(dataset):\n",
        "            source_text = example['translation'][source_lang]\n",
        "            reference = example['translation'][target_lang]\n",
        "\n",
        "            try:\n",
        "                translation = translate_text(source_text, model_name, source_lang, target_lang)\n",
        "\n",
        "                source_texts.append(source_text)\n",
        "                translations.append(translation)\n",
        "                references.append(reference)\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {model_name} on text: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if translations:  # Only calculate metrics if we have translations\n",
        "            # Calculate all metrics\n",
        "            metrics = calculate_metrics(references, translations)\n",
        "            results[f\"{model_name} ({model_info['provider']})\"] = metrics\n",
        "\n",
        "            # Save translations and source texts for review\n",
        "            pd.DataFrame({\n",
        "                'Source': source_texts,\n",
        "                'Reference': references,\n",
        "                'Translation': translations\n",
        "            }).to_csv(f'translations_{model_name}_{source_lang}-{target_lang}.csv', index=False)\n",
        "\n",
        "    return pd.DataFrame(results).T\n",
        "\n",
        "def visualize_results(results, pair_name):\n",
        "    \"\"\"Create visualizations for the evaluation results.\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    sns.heatmap(results, annot=True, cmap='YlOrRd', fmt='.3f')\n",
        "    plt.title(f'Translation Metrics Comparison for {pair_name}')\n",
        "    plt.ylabel('Models')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'mt_evaluation_heatmap_{pair_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Language pairs to evaluate\n",
        "    language_pairs = [\n",
        "    (\"cs-en\", \"Czech-English\"),\n",
        "    (\"de-en\", \"German-English\"),\n",
        "    (\"fi-en\", \"Finnish-English\"),\n",
        "    (\"fr-de\", \"French-German\"),\n",
        "    (\"gu-en\", \"Gujarati-English\"),\n",
        "    (\"kk-en\", \"Kazakh-English\"),\n",
        "    (\"lt-en\", \"Lithuanian-English\"),\n",
        "    (\"ru-en\", \"Russian-English\"),\n",
        "    (\"zh-en\", \"Chinese-English\")\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for pair_code, pair_name in language_pairs:\n",
        "        print(f\"\\nEvaluating {pair_name} translations...\")\n",
        "\n",
        "        # Load dataset\n",
        "        dataset = load_translation_data(pair_code, num_samples=10000)\n",
        "\n",
        "        # Get source and target language codes\n",
        "        source_lang, target_lang = pair_code.split(\"-\")\n",
        "\n",
        "        # Evaluate models\n",
        "        results = evaluate_models(dataset, source_lang, target_lang)\n",
        "\n",
        "        # Save results\n",
        "        results.to_csv(f\"mt_evaluation_results_{pair_code}.csv\")\n",
        "\n",
        "        # Create visualizations\n",
        "        visualize_results(results, pair_name)\n",
        "\n",
        "        # Store results\n",
        "        all_results[pair_name] = results\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nResults for {pair_name}:\")\n",
        "        print(results)\n",
        "\n",
        "    # Create combined visualization\n",
        "    plt.figure(figsize=(25, 12))\n",
        "    for idx, (pair_name, results) in enumerate(all_results.items()):\n",
        "        plt.subplot(1, 2, idx+1)\n",
        "        sns.heatmap(results, annot=True, cmap='YlOrRd', fmt='.3f')\n",
        "        plt.title(f'Results for {pair_name}')\n",
        "        plt.ylabel('Models')\n",
        "        plt.xlabel('Metrics')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('combined_results.png')\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-07T21:31:56.387893Z",
          "iopub.execute_input": "2024-11-07T21:31:56.388515Z"
        },
        "id": "dRuJSOJFRbxm"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
